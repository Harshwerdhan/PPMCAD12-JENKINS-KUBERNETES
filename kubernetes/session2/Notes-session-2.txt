---------------------------
Kubernetes Traning Program
---------------------------

-------------
Session 2:
-------------

How does Kubernetes cluster components Work Together:
-----------------------------------------------------

Control Plane and multiple worker nodes

Component of Control Plane:

- User/Admins interacts with the cluster through the API server
- API server validates and processes the request
- Scheduler assigns Pods to Nodes
- Controller manager ensures desired state is maintained
- kubelet (runs on both control plane and worker node) communicates with the API server and manages containers on the node
- kube-proxy manages network communication for Pods
- etcd saves the current state of the cluster



1.) Installting a self managed Kubernetes Cluster:
--------------------------------------------------

KUBEADM: It is a tool specifically developed to provision self managed Kubernetes cluster


Steps:

a.) Provision 3 virtual machines which would be a part of your cluster
    - I have used terraform to provision 3 EC2 server on aws
    - Reference scripts: Self-Managed-Kuberenetes-Cluster\tf-code

b.) After the EC2 instances are provisioned, access them (ssh using your pem file)

c.) Use the k8s-master.sh script to provision the master node
    - Reference script: Self-Managed-Kuberenetes-Cluster\shell-scripts\k8s-master.sh

d.) Use the k8s-worker.sh script to provision the worker node
    - Reference script: Self-Managed-Kuberenetes-Cluster\shell-scripts\k8s-worker.sh

e.) Merge the Kubeconfig file to your local workstation (which will mostly be your laptop)
    - Run below mentioned commands preferable using git bash in windows laptop:
        - cd /c/Users/<your-username>/.kube
        - scp username@master_node_ip:/etc/kubernetes/admin.conf self-managed-cluster-config

        Via Linux:
        - KUBECONFIG=/c/Users/<your-username>/.kube/config:/c/Users/<your-username>/.kube/self-managed-cluster-config kubectl config view --flatten > config-merged
        - mv config-merged config

        Via PowerShell:
        - $env:KUBECONFIG = "$PWD\config;$PWD\self-managed-cluster.txt"
        - kubectl config view --flatten | Out-File -FilePath "$PWD\config-merged" -Encoding utf8
        - mv config-merged config
        # Backup old config
        - Rename-Item "$env:USERPROFILE\.kube\config" "config-backup"
        # Replace with the merged config
        - Move-Item ".\config-merged" "$env:USERPROFILE\.kube\config"

f.) Check and swicth the context and further check the cluster status:
    - Run below command:
        - kubectl config get-contexts
        - kubectl config use-context kubernetes-admin@kubernetes
        - kubectl get pods -A

Additonal Scripts to reset master and worker nodes, if provisioned from incorrect configuration added in the folder: Self-Managed-Kuberenetes-Cluster\shell-scripts

Some questions for interview:

Question.) How you can run the pod without Scheduler?

Ans:

Running a pod without the scheduler involves using static pods or manually specifying the node where the pod should run. 

Here are two main approaches:

a.) Using Static Pods:

Static pods are managed directly by the kubelet on a specific node, without the involvement of the API server or the scheduler.

When you run kubeadm init, it sets up the control plane components of a Kubernetes cluster. 

Here's an overview of how the various components are started and where you can find their manifest files:

Manifest file location:
The static Pod manifest files for the control plane components are typically located in the /etc/kubernetes/manifests/ directory on the control plane node.

Component startup process:
kubeadm generates the necessary manifest files for core components.
These manifests are placed in the /etc/kubernetes/manifests/ directory.
The kubelet, which should already be running on the node, watches this directory.
When the kubelet detects new or changed manifests, it creates static Pods based on these manifests.

b.) Pod Specification with NodeName:

e.g.:
apiVersion: v1
kind: Pod
metadata:
  name: nginx-on-specific-node
spec:
  containers:
  - name: nginx
    image: nginx
  nodeName: your-node-name


Important points to remember:
- Static pods are created and managed by the kubelet, not the API server.
- Pods with nodeName are still managed by the API server but bypass the scheduling process.
- Both methods limit the flexibility and automatic rescheduling capabilities of Kubernetes.
- These approaches are typically used for system pods or in specific scenarios where you need precise control over pod placement.


2.) Installting Elastic Kubernetes Service (EKS) Cluster on AWS:
----------------------------------------------------------------

Steps:

a.) Provision the EKS cluster manually or via the Terraform code // Request everyone to create the cluster manually
    - I have used terraform to provision the cluster on aws
    - Reference scripts: EKS\tf-code
    - This script will provision the EKS cluster and associate the Node Group with it
    - So, basically a running cluster is available for us

b.) you should have awscli installed on your system to work with EKS
    - Run below command to install it:
    - msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi
    - aws --version // to check the version

b.) Fetch EKS Kubeconfig file to your local workstation
    - Run below mentioned command from powershell
        - aws sts get-caller-identity // To verify your current user access on AWS
        - aws eks update-kubeconfig --region <region-code> --name <cluster-name>
        - kubectl get svc // To test the configuration


Discussions during the class:
-----------------------------

AWS VPC & subnets
------------------

VPC:
Contains one or more subnets (public and private).
Choose a non-overlapping CIDR (example: 10.0.0.0/16).

Public subnet:
- Route table must have a route to the Internet Gateway (IGW) (e.g. 0.0.0.0/0 -> igw-xxxx).
- Instances launched here can have public IPv4 addresses (or an Elastic IP).
- Use for bastion hosts, NAT gateways (in AWS), and public-facing load balancers.

Private subnet:
- Route table should not contain a route to an IGW.
- For outbound internet access (e.g., for updating any packages like java, nginx etc. or pulling container images), traffic should be routed to a NAT gateway which resides in a public subnet. Example: 0.0.0.0/0 -> nat-xxxx in private subnet route table.
- Use private subnets for application servers, databases, and other non-public resources.

Access patterns:

Accessing an instance in a public subnet:
    - SSH directly to the instance’s public IP (subject to security group rules).
    - Prefer restricting SSH access by IP range (security group) — do not open 0.0.0.0/0 unless unavoidable.

Accessing an instance in a private subnet:
    - Typical flow:
        - SSH to a bastion/jump host in the public subnet (bastion has public IP).
        - From the bastion, SSH to the private instance via its private IP.
        - Ensure Security Groups allow SSH from the bastion’s SG to private instances


Auto-scaling (ASG) basics:
--------------------------

min - minimum number of instances the ASG will maintain (never goes below).
max - maximum number of instances the ASG may scale up to.
desired - current target capacity of the ASG (initial size). ASG will try to maintain this unless scaling policies change it.

Typical example: min=2, desired=3, max=6 for a resilient application tier.

Kubernetes-specific worker nodes placement:
-------------------------------------------

- Put worker nodes in private subnets (no public IP), with NAT for outbound access
- control-plane can be public or private depending on design.
- Use ELB/ALB in public subnets for external traffic
- When creating managed node group for k8s, ensure subnets span multiple AZs for HA.